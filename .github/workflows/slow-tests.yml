name: Slow Test Review

on:
  # every Monday at 04:00 UTC (after Sunday's update.yml at 02:36 UTC)
  # https://crontab.guru/#0_4_*_*_1
  schedule:
    - cron: 0 4 * * 1

  workflow_dispatch:
    inputs:
      dry_run:
        description: Dry run - do not create/modify issues
        required: false
        type: boolean
        default: false

permissions: {}

concurrency:
  # only allow one run at a time to avoid duplicate issues
  group: ${{ github.workflow }}
  cancel-in-progress: true

env:
  # TODO: Replace with the actual epic issue number
  EPIC_NUMBER: '15637'
  # Maximum number of incomplete sub-issues before we stop adding more
  MAX_INCOMPLETE: '1'
  # Platforms to fetch duration files for
  PLATFORMS: |
    Linux
    macOS
    Windows
  GH_TOKEN: ${{ secrets.AUTO_REPORT_TEST_FAILURE }}
  ISSUE_TEMPLATE: |
    [slow-tests.yml]: ${WORKFLOW_URL}

    ## Slow Test Review

    | Test | `${TEST_NAME}` |
    |---|---|
    | **Duration** | ${DURATION}s |
    | **Platform** | ${PLATFORM} |
    | **Identified** | ${TODAY} |
    | **Parent Epic** | #${EPIC_NUMBER} |

    ## Task

    This test has been identified as one of the slowest in the conda test suite.

    1. **Investigate** why this test is slow
    2. **Optimize** the test (see tips below) or **document** why it cannot be improved
    3. **Submit a PR** with your changes
    4. **Close this issue** when complete

    <details>
    <summary>Optimization tips</summary>

    - Share expensive setup/teardown using fixtures
    - Mock I/O, network calls, or other slow operations
    - Consolidate parametrized tests where possible
    - Profile the test to identify bottlenecks

    </details>

    ###### Auto-generated by the [`slow-tests.yml`][slow-tests.yml] workflow, see ${RUN_URL}.

jobs:
  review:
    if: '!github.event.repository.fork'
    runs-on: ubuntu-slim
    steps:
      - name: Get epic sub-issues
        id: epic
        run: |
          # Verify epic exists, default to dry-run if missing
          if ! gh issue view "$EPIC_NUMBER" --json number > /dev/null 2>&1; then
            echo "::warning::Epic #${EPIC_NUMBER} does not exist. Running in dry-run mode."
            echo "epic_missing=true" >> "$GITHUB_OUTPUT"
            echo "incomplete_count=0" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          # Get sub-issues of the epic
          # jq: extract sub-issue number, state, and title as tab-separated values
          gh issue view "$EPIC_NUMBER" \
            --json subIssues \
            --jq '.subIssues[] | "\(.number)\t\(.state)\t\(.title)"' > /tmp/sub_issues.txt || true

          echo "Current sub-issues:"
          cat /tmp/sub_issues.txt || echo "(none)"

          # Extract test names from sub-issue titles (format: "Review slow test: <test_name>")
          cut -f3 /tmp/sub_issues.txt | sed 's/^Review slow test: //' > /tmp/tracked_tests.txt || true

          echo "Previously reported tests (excluded from future reports):"
          cat /tmp/tracked_tests.txt || echo "(none)"

          # Count open (incomplete) sub-issues
          incomplete_count=$(grep -c $'\tOPEN\t' /tmp/sub_issues.txt || echo 0)
          echo "Incomplete sub-issues: $incomplete_count"
          echo "incomplete_count=$incomplete_count" >> "$GITHUB_OUTPUT"

      - name: Fetch duration files
        id: durations
        env:
          BASE_URL: https://raw.githubusercontent.com/${{ github.repository }}/main/durations
        run: |
          # Combine all durations, keeping max and tracking which platform
          # Format: {test_name: {duration: N, platform: "X"}}
          echo "{}" > /tmp/all_durations.json
          fetched=""
          missing=""

          while IFS= read -r platform; do
            [[ -z "$platform" ]] && continue
            url="${BASE_URL}/${platform}.json"
            echo "Fetching $url"

            # Fetch duration files directly from GitHub
            curl -fsSL "$url" -o "/tmp/${platform}.json" 2>/dev/null || {
              echo "::warning::Failed to fetch $platform durations"
              missing="${missing:+$missing, }$platform"
              continue
            }

            fetched="${fetched:+$fetched, }$platform"

            # Merge durations keeping the max duration + platform
            # jq: for each test, keep the max duration across platforms
            # and record which platform had that max ensuring we report the
            # slowest instance of each test regardless of platform
            # Output format: {test_name: {duration: N, platform: "X"}, ...}
            jq -s --arg platform "$platform" '
              .[0] as $existing |
              .[1] as $new |
              ($existing | keys) + ($new | keys) | unique |
              map(. as $k |
                ($existing[$k].duration // 0) as $old_dur |
                ($new[$k] // 0) as $new_dur |
                if $new_dur > $old_dur then
                  {($k): {duration: $new_dur, platform: $platform}}
                else
                  {($k): ($existing[$k] // {duration: 0, platform: $platform})}
                end
              ) | add
            ' /tmp/all_durations.json "/tmp/${platform}.json" > /tmp/merged.json
            mv /tmp/merged.json /tmp/all_durations.json
          done <<< "$PLATFORMS"

          echo "platforms_fetched=${fetched:-none}" >> "$GITHUB_OUTPUT"
          echo "platforms_missing=${missing}" >> "$GITHUB_OUTPUT"

      - name: Create sub-issues up to cap
        id: create
        env:
          DRY_RUN: ${{ inputs.dry_run || steps.epic.outputs.epic_missing }}
          INCOMPLETE_COUNT: ${{ steps.epic.outputs.incomplete_count }}
        run: |
          to_add=$((MAX_INCOMPLETE - INCOMPLETE_COUNT))
          if [[ "$to_add" -le 0 ]]; then
            echo "Already at cap (${INCOMPLETE_COUNT} incomplete sub-issues)"
            echo "created_count=-1" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          echo "Will create up to $to_add new sub-issues"
          created=0
          today=$(date +%Y-%m-%d)

          # Get tracked tests for exclusion
          # jq: convert newline-separated text file to JSON array for filtering
          tracked_tests=$(cat /tmp/tracked_tests.txt 2>/dev/null | jq -R -s 'split("\n") | map(select(length > 0))' || echo '[]')

          # Get slowest untracked tests sorted by duration (with platform)
          # jq: filter out already-tracked tests, sort by duration descending,
          # output as tab-separated: test_name, duration, platform
          jq -r --argjson tracked "$tracked_tests" '
            to_entries |
            map(select(.key as $k | $tracked | index($k) | not)) |
            sort_by(-.value.duration) |
            .[] |
            "\(.key)\t\(.value.duration)\t\(.value.platform)"
          ' /tmp/all_durations.json > /tmp/untracked_tests.txt

          while IFS=$'\t' read -r test_name duration platform; do
            [[ "$created" -ge "$to_add" ]] && break
            [[ -z "$test_name" ]] && continue

            # Create issue content using envsubst
            TEST_NAME="$test_name" \
            DURATION="$duration" \
            PLATFORM="$platform" \
            TODAY="$today" \
            WORKFLOW_URL="${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/blob/main/.github/workflows/slow-tests.yml" \
            RUN_URL="${GITHUB_SERVER_URL}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}" \
            envsubst <<< "$ISSUE_TEMPLATE" > /tmp/slow-test-issue.md

            if [[ "$DRY_RUN" == "true" ]]; then
              echo "::group::Would create: Review slow test: $test_name"
              cat /tmp/slow-test-issue.md
              echo "::endgroup::"
            else
              # Create sub-issue linked to the epic
              # jq: extract the newly created issue number from JSON response
              issue_number=$(gh issue create \
                --title "Review slow test: $test_name" \
                --label "type::testing" \
                --body-file /tmp/slow-test-issue.md \
                --parent-issue "$EPIC_NUMBER" \
                --json number --jq '.number')

              echo "Created sub-issue #$issue_number"
            fi

            ((created++))
          done < /tmp/untracked_tests.txt

          echo "Processed $created tests"
          echo "created_count=$created" >> "$GITHUB_OUTPUT"

      - name: Summary
        if: always()
        env:
          EPIC_MISSING: ${{ steps.epic.outputs.epic_missing }}
          DRY_RUN: ${{ inputs.dry_run || steps.epic.outputs.epic_missing }}
          INCOMPLETE_COUNT: ${{ steps.epic.outputs.incomplete_count }}
          CREATED_COUNT: ${{ steps.create.outputs.created_count }}
          PLATFORMS_FETCHED: ${{ steps.durations.outputs.platforms_fetched }}
          PLATFORMS_MISSING: ${{ steps.durations.outputs.platforms_missing }}
        run: |
          echo "## Slow Test Review Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [[ "${EPIC_MISSING}" == "true" ]]; then
            echo "**Epic:** #${EPIC_NUMBER} (not found, running as dry run)" >> $GITHUB_STEP_SUMMARY
          else
            echo "**Epic:** #${EPIC_NUMBER}" >> $GITHUB_STEP_SUMMARY
          fi
          echo "**Platforms:** ${PLATFORMS_FETCHED}" >> $GITHUB_STEP_SUMMARY
          if [[ -n "${PLATFORMS_MISSING}" ]]; then
            echo "**Missing:** ${PLATFORMS_MISSING}" >> $GITHUB_STEP_SUMMARY
          fi
          echo "**Incomplete sub-issues before:** ${INCOMPLETE_COUNT}" >> $GITHUB_STEP_SUMMARY
          echo "**Max allowed:** ${MAX_INCOMPLETE}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [[ "${CREATED_COUNT}" == "-1" ]]; then
            echo "**Already at cap, no new sub-issues created**" >> $GITHUB_STEP_SUMMARY
          elif [[ "${CREATED_COUNT}" == "0" ]]; then
            echo "**No untracked slow tests found**" >> $GITHUB_STEP_SUMMARY
          elif [[ "${DRY_RUN}" == "true" ]]; then
            echo "**Dry run, would create ${CREATED_COUNT} new sub-issue(s)**" >> $GITHUB_STEP_SUMMARY
          else
            echo "**Created ${CREATED_COUNT} new sub-issue(s)**" >> $GITHUB_STEP_SUMMARY
          fi
